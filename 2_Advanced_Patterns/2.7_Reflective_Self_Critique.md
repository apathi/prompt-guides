# 2.7 Reflective / Self‑Critique & Chain‑of‑Verification

Add a quick **self‑check pass**: ask the model to critique its own draft against rubric and fix issues; optionally add a **verification step** that confirms citations and schema. [O2][A4]

```plain text
Draft the answer, then:
- Critique: Is format valid? Are claims cited? Any hedging needed?
- Revise: Fix issues; ensure schema compliance.
- Verify: If any claim lacks support in provided sources, replace with "I don't know."
```

## Why Self-Critique Works

- **Quality improvement**: Catches errors before final output
- **Consistency checking**: Ensures adherence to format requirements
- **Fact verification**: Validates claims against provided sources
- **Completeness validation**: Checks all requirements are met

## Implementation Patterns

### Basic Self-Check
```plain text
After generating your response:
1. Check format compliance
2. Verify all claims have citations
3. Ensure tone matches requirements
4. Fix any issues found
```

### Multi-Pass Verification
```plain text
Step 1 - Draft: Generate initial response
Step 2 - Critique: Review against rubric:
  - Format: Matches required schema?
  - Facts: All claims supported by sources?
  - Style: Appropriate tone and length?
  - Safety: No policy violations?
Step 3 - Revise: Fix identified issues
Step 4 - Final check: Confirm all requirements met
```

### Chain-of-Verification
```plain text
Verification protocol:
1. List all factual claims made
2. For each claim, identify supporting source
3. If no source found, mark as "needs verification"
4. Replace unsupported claims with "I don't know" or remove
5. Double-check citations are accurate
```

## Provider-Specific Applications

### Anthropic Approach [A4]
- **Chain of Thought**: Multi-pass reasoning with reflection
- **Self-correction**: Model reviews and improves its own output
- **Source validation**: Checks citations against provided documents

### OpenAI Approach [O2]
- **Critique passes**: Explicit review steps in agent workflows
- **Verification steps**: Structured checking against requirements
- **Iterative refinement**: Multiple rounds of improvement

## Best Practices

1. **Define clear rubrics**: Specific criteria for self-evaluation
2. **Structure the process**: Clear steps for critique and revision
3. **Focus on key issues**: Format, facts, style, safety
4. **Make it actionable**: Specific guidance on what to fix
5. **Validate improvements**: Ensure revisions actually address issues

## Example Implementation

```plain text
<self_critique_process>
1. Generate initial response
2. Self-evaluate:
   - Format check: Does output match required schema?
   - Fact check: Are all claims supported by provided sources?
   - Style check: Appropriate tone, length, and structure?
   - Completeness: All requirements addressed?
3. If issues found, revise and re-check
4. Final validation: Confirm response meets all criteria
</self_critique_process>
```