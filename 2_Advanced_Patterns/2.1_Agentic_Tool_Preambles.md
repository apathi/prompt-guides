# 2.1 Agentic Tool Preambles (OpenAI) + Tool Use (Anthropic)

**Concept: Agentic Behavior**
- An *agent* isn't just answering; it can **plan, act, and reflect**.
- In modern APIs (OpenAI GPT-5, Anthropic Claude, Gemini function calling), the model can **call tools** — e.g., search, calculator, database query — instead of just generating free text.
- The **preamble** tells the model *how to behave as an agent*, so it doesn't "go rogue" or waste calls.
- **Why:** Clear upfront plans + progress updates improve user trust in agent workflows.

**Why Agentic Tool Preambles Matter**
- **Predictability**: Without a preamble, models may overuse tools, skip steps, or hallucinate results.
- **Transparency**: Narrating steps gives users confidence that the model isn't "black box guessing."
- **Safety**: Limits ("stop after 2 calls") prevent runaway loops or unsafe queries.
- **Control**: You can specify *when to use a tool, how often, and what to do if it fails*.

## How Each Provider Approaches This

<details>
<summary><b>OpenAI (GPT-5)</b> [O2]</summary>

- Introduced **tool preambles** (plan + narrate steps).
- `reasoning_effort` parameter adjusts how deeply the model reasons before/while using tools. i.e modulate proactivity; define **stop conditions** and **safe vs unsafe actions**
- Works with the **Responses API**: you can define a tool schema, and the model will call it when needed.

**Using reasoning_effort in an Agentic workflow**

**Scenario:** You're building an agent that must **find the most recent fine amount and cause** related to a company's GDPR enforcement, then return a concise, sourced summary. The agent has two tools:
- search(q: str): gets recent web snippets
- calc(expr: str): does math if you need currency conversion/aggregation

---

<details>
<summary><b>A) High exploration (deep reasoning, multi-step planning)</b></summary>

Use when accuracy and completeness matter more than latency/cost; allow a plan, one or two tool calls, and require citations. OpenAI's GPT-5 guide recommends *increasing* reasoning effort for multi-step/agentic flows; it also pairs well with a tool preamble and explicit stop conditions.

```typescript
<tool_preambles>
- Restate: "Find the most recent GDPR fine amount for Meta and summarize the cause."
- Plan:
  1) Use search("Meta GDPR fine 2024 2025 amount") to confirm the latest event.
  2) Extract fine amount + cause from top credible sources.
  3) If needed, use calc() to normalize currency.
- Narrate steps briefly as you work.
- Stop after ≤2 tool calls OR once a reliable citation with amount + cause is found.
- If still uncertain after tool budget: say "I couldn't confirm." and list what's missing.
</tool_preambles>

# Controls
- reasoning_effort: high   # prioritize deeper exploration & planning
- tool_budget: 2
- stop_condition: {found_amount: true AND found_citation: true}

# Output format
- 2–3 sentence summary (≤60 words)
- One bullet with the exact amount and year
- One bullet with a citation (publisher + date)
```

**Why this works**
- The **preamble** raises planning quality and transparency (users see what the agent is doing).
- **High effort** tends to explore more candidate explanations/sources before committing (useful for correctness).
- The **stop condition** and **tool budget** prevent runaway calls. (OpenAI's GPT-5 prompting guide discusses balancing exploration with limits; the "New Params & Tools" note calls out effort tradeoffs.)

</details>

<details>
<summary><b>B) Medium exploration (balanced)</b></summary>

Use for general knowledge tasks where you still want planning, but you're optimizing a bit more for **latency**. The GPT-5 guide notes many workflows are fine at **medium** effort.

```typescript
<tool_preambles>
- Goal: confirm latest GDPR fine amount + cause for Meta; return concise, sourced answer.
- Plan: Try a single search; only call calc() if currency normalization is required.
- If search is inconclusive, state uncertainty and stop.
</tool_preambles>

# Controls
- reasoning_effort: medium
- tool_budget: 1
- stop_condition: {found_amount: true AND found_citation: true}

# Output format
- One 40–60 word paragraph + 1 citation
```

**Why this works**
- **Medium** effort often gives reliable results with lower latency/cost.
- Keeping **tool_budget = 1** limits overhead while still enabling a single fact-check.

</details>

<details>
<summary><b>C) Minimal/Low exploration (fast path for deterministic tasks)</b></summary>

For extraction/formatting or when you're confident the answer is already in **provided context** (no web needed). Cookbook examples call out **"minimal"** (or lower effort) to reduce exploration depth and improve latency.

```typescript
# Context
"""
[Paste trusted internal memo / official press release snippet here]
"""

<tool_preambles>
- Do not call external tools.
- Extract the fine amount and cause strictly from the provided Context.
- If amount or cause is not present, say "I don't know."
</tool_preambles>

# Controls
- reasoning_effort: minimal   # or low, depending on the API option
- tool_budget: 0
- stop_condition: n/a

# Output format
- "Amount: …; Cause: …; Source: internal memo §X.Y"
```

**Why this works**
- **Minimal/low** effort sharply reduces exploration/latency and avoids unnecessary tool calls—ideal for **formatting or source-bounded** tasks.

</details>

<details>
<summary><b>D) Adaptive pattern: escalate if uncertain</b></summary>

Combine speed + quality: start fast at minimal/low effort; if you can't confirm, **automatically escalate** to medium or high, and allow tools.

```typescript
# Phase 1 (fast):
- reasoning_effort: minimal
- tool_budget: 0
- Try to answer from provided Context only.

If Phase 1 returns "I don't know" → Phase 2:
# Phase 2 (balanced):
- reasoning_effort: medium
- tool_budget: 1
- Use search() once to confirm amount + cause; return citation.

If still uncertain → Phase 3 (thorough):
- reasoning_effort: high
- tool_budget: 2
- Plan + search; calc() if needed; otherwise end with explicit uncertainty + what's missing.
```

**Why this works**
- Matches OpenAI guidance: **use lower effort when possible** to save cost/latency; escalate only if needed.

</details>

**Notes on implementation (SDK/API)**
- **Values & intent**: OpenAI's docs discuss **tuning the model's exploration depth** with a *reasoning effort* control and show that lower effort reduces latency; some docs also mention a **"minimal"** option for GPT-5 models. Exact parameter names can vary by endpoint/SDK; check the current **Cookbook** and **Platform** docs for the syntax you're using.
- **Pair with the Responses API**: When you need structured outputs or persistent multi-turn reasoning with tools, combine your prompt + tool schemas with Responses.

**What to measure in your evals**
- **Correctness** (amount & cause match a trusted source).
- **Latency** (lower with minimal/low; higher with high effort).
- **Tool calls used** (stay within budget; avoid unnecessary calls).
- **Citations present & credible** (publisher/date present).

Guidance from OpenAI emphasizes iterating with evals and lowering effort for consistent workflows.

</details>

<details>
<summary><b>Anthropic (Claude)</b> [A12]</summary>

- Uses `tool_use`**→ **`tool_result`.
- You define tool specs (name, description, JSON schema).
- Claude decides when to emit a tool_use; you run the tool and feed back a tool_result.
- Supports **parallel tool calls**.

**How Anthropic Tool Use Works**

Anthropic's Claude models use a **two-step loop** for tool interaction:

1. **Model emits a tool_use block**
	- Inside its response, Claude signals that it wants to call a tool.
	- The block includes:
		- **tool name** (must match a spec you provided),
		- **parameters** (conforming to your JSON schema).
2. **You (the developer) execute the tool**
	- Run the API, database query, or function.
	- Gather the output.
3. **You feed back a tool_result block**
	- This goes into the next Claude message, tagged with the **same id** that the model used in tool_use.
	- Claude then continues reasoning with that new information.

🔄 This loop repeats until Claude finishes the task or you enforce a stop condition.

**📦 Defining Tool Specs**

You pass Claude a **list of tool specifications** when you start the conversation. Each tool has:
- name: unique identifier ("search_web", "calculator")
- description: short natural-language summary (guides when Claude should use it)
- input_schema: JSON Schema defining expected fields

Example spec for tools you'd register with the Claude API:

```typescript
{
  "name": "search_web",
  "description": "Search the web for recent information.",
  "input_schema": {
    "type": "object",
    "properties": {
      "query": { "type": "string" }
    },
    "required": ["query"]
  }
}
```

**⚡ Tool Use Flow Example**

**Step 1. User prompt:**
```typescript
What is the latest fine Meta received under GDPR and why?
```

**Step 2. Claude response (emits a tool call):**
```typescript
{
  "id": "toolu_1",
  "type": "tool_use",
  "name": "search_web",
  "input": {
    "query": "Meta GDPR fine 2024 amount cause"
  }
}
```

**Step 3. Your system executes the search tool** and gets snippets, e.g.:
> "Meta fined €1.2B in 2024 for unlawful data transfers to the U.S."

**Step 4. You return tool_result:**
```typescript
{
  "id": "toolu_1",
  "type": "tool_result",
  "content": "Meta fined €1.2 billion in 2024 for data transfers to the US."
}
```

**Step 5. Claude continues reasoning:**
```typescript
Meta was fined €1.2B in 2024 for transferring EU user data to the US without adequate safeguards. This was the largest GDPR fine to date.
```

## **🔀 Parallel Tool Calls**

Claude can issue **multiple tool_use blocks in a single turn**.

For example, in an analysis task it might:

```typescript
[
  {
    "id": "toolu_1",
    "type": "tool_use",
    "name": "search_web",
    "input": {"query": "Meta GDPR fine 2024"}
  },
  {
    "id": "toolu_2",
    "type": "tool_use",
    "name": "currency_converter",
    "input": {"amount": 1200000000, "from": "EUR", "to": "USD"}
  }
]
```

You then run both tools concurrently and return two tool_result objects in the next message. Claude merges those into its reasoning.

---

## **⚖️ Why This Matters**
- **Declarative control**: You define tools up front; Claude chooses when to use them.
- **Safety**: Schema validation ensures inputs are well-formed.
- **Scalability**: Parallel calls speed up multi-tool workflows.
- **Transparency**: The tool_use → tool_result chain makes it easy to debug.

---

## **🛠️ Best Practices**
1. **Describe tools carefully**: The description heavily influences whether Claude decides to call it.
2. **Keep schemas tight**: Constrain inputs with enums, required fields, ranges.
3. **Handle errors gracefully**: If a tool fails, return a tool_result with an error message so Claude can adapt.
4. **Set budgets**: Max number of tool calls per request (to prevent loops).
5. **Test parallelization**: Ensure your infra can handle concurrent tool calls.

---

✅ So in short: Anthropic's tool_use → tool_result flow is a clean contract where *you own execution* and *Claude owns orchestration*. It differs from OpenAI's **Responses API** tool-calling in syntax but the spirit is the same: clear specs, structured outputs, loop until done.

</details>

<details>
<summary><b>Gemini</b></summary>

- Similar to OpenAI/Anthropic, but emphasizes **safety overrides** and configurable thresholds when tools interact with the outside world.

</details>

## Agentic Tool Preamble Template

```plain text
<tool_preambles>
- Restate the user's goal succinctly.
- Outline a stepwise plan before any tool calls.
- While executing, narrate each step briefly; mark progress.
- End by summarizing completed work vs. plan.
</tool_preambles>
```

Another example - A compliance-style agent with **search** and **calculator** tool

```plain text
<tool_preambles>
- Restate user goal: "Find the most recent GDPR fine amount for Meta and summarize the cause."
- Plan:
  1) Use search(q) to look up "Meta GDPR fine 2024 amount".
  2) Extract fine amount and cause from snippets.
  3) Use calculator(expr) if needed to adjust currency or totals.
- While executing: explain each step briefly.
- Stop after 2 tool calls; if still uncertain, respond "I couldn't confirm."
- End with: summary of findings + citation.
</tool_preambles>

# Tools
- search(q: str): web snippets from recent sources
- calculator(expr: str): evaluate math expressions
```

**Safety Budget (constrain eagerness)**

```plain text
- Max tool calls: 2
- Stop once you can name exact change or cite source.
- Proceed under uncertainty only if noted to user post‑hoc.

- Never perform calculator calls on personal identifiers (e.g., phone numbers).
- If search results are older than 2023, warn user and stop.
- Do not summarize content flagged as disallowed (e.g., leaked data).
```